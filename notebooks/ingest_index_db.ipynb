{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/whysocurious/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables from the .envrc file\n",
    "load_dotenv('../.envrc')\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def read_letters(letters_folder):\n",
    "#     letters = []\n",
    "#     for filename in os.listdir(letters_folder):\n",
    "#         if filename.endswith('.txt'):\n",
    "#             filepath = os.path.join(letters_folder, filename)\n",
    "#             try:\n",
    "#                 # Try opening with utf-8 encoding\n",
    "#                 with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#                     content = file.read()\n",
    "#                     letters.append({\n",
    "#                         'filename': filename,\n",
    "#                         'content': content\n",
    "#                     })\n",
    "#             except UnicodeDecodeError:\n",
    "#                 # If utf-8 fails, try with ISO-8859-1 encoding (Latin-1)\n",
    "#                 with open(filepath, 'r', encoding='ISO-8859-1') as file:\n",
    "#                     content = file.read()\n",
    "#                     letters.append({\n",
    "#                         'filename': filename,\n",
    "#                         'content': content\n",
    "#                     })\n",
    "#     return letters\n",
    "\n",
    "# letters_folder = '../data/letters'\n",
    "# letters_data = read_letters(letters_folder)\n",
    "\n",
    "# def read_reports(reports_folder):\n",
    "#     reports = []\n",
    "#     for filename in os.listdir(reports_folder):\n",
    "#         if filename.endswith('.pdf'):\n",
    "#             filepath = os.path.join(reports_folder, filename)\n",
    "#             with open(filepath, 'rb') as file:\n",
    "#                 reader = PyPDF2.PdfReader(file)\n",
    "#                 text = ''\n",
    "#                 for page_num in range(len(reader.pages)):\n",
    "#                     page = reader.pages[page_num]\n",
    "#                     text += page.extract_text()\n",
    "#                 reports.append({\n",
    "#                     'filename': filename,\n",
    "#                     'content': text\n",
    "#                 })\n",
    "#     return reports\n",
    "\n",
    "# reports_folder = '../data/reports'\n",
    "# reports_data = read_reports(reports_folder)\n",
    "\n",
    "\n",
    "def read_papers(papers_folder):\n",
    "    papers = []\n",
    "    for filename in os.listdir(papers_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            # print (filename)\n",
    "            filepath = os.path.join(papers_folder, filename)\n",
    "            with open(filepath, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page_num in range(len(reader.pages)):\n",
    "                    page = reader.pages[page_num]\n",
    "                    title = reader.metadata\n",
    "                    text += page.extract_text()\n",
    "                papers.append({\n",
    "                    'filename': filename,\n",
    "                    'metadata':title,\n",
    "                    'content': text\n",
    "                })\n",
    "    return papers\n",
    "\n",
    "papers_folder = '../data/papers'\n",
    "papers_data = read_papers(papers_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove unwanted characters and normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text, max_tokens=700):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk.split()) + len(sentence.split()) <= max_tokens:\n",
    "            current_chunk += ' ' + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_letters = []\n",
    "# for letter in letters_data:\n",
    "#     text = preprocess_text(letter['content'])\n",
    "#     chunks = chunk_text(text)\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         processed_letters.append({\n",
    "#             'source': 'letter',\n",
    "#             'year': letter['filename'].split(\" - \")[1][:4],\n",
    "#             'chunk_id': f\"{letter['filename'].split(' - ')[1][:4]}_chunk_{i}\",\n",
    "#             'content': chunk\n",
    "#         })\n",
    "\n",
    "# processed_reports = []\n",
    "# for report in reports_data:\n",
    "#     text = preprocess_text(report['content'])\n",
    "#     chunks = chunk_text(text)\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         processed_reports.append({\n",
    "#             'source': 'report',\n",
    "#             'ticker': report['filename'].split(\"_\")[0],\n",
    "#             'year': report['filename'].split(\"_\")[1][:4],\n",
    "#             'chunk_id': f\"{report['filename'].split('.')[0]}_chunk_{i}\",\n",
    "#             'content': chunk\n",
    "#         })\n",
    "\n",
    "\n",
    "processed_papers = []\n",
    "for paper in papers_data:\n",
    "    text = preprocess_text(paper['content'])\n",
    "    chunks = chunk_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        processed_papers.append({\n",
    "            'source': 'paper',\n",
    "            'chunk_id': f\"{paper['filename'].split('.')[0]}_chunk_{i}\",\n",
    "            'content': chunk\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668\n"
     ]
    }
   ],
   "source": [
    "# print (len(processed_letters), len(processed_reports))\n",
    "print ( len(processed_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_metadata(chunk_text):\n",
    "#     prompt = (\n",
    "#         \"Analyze the following text and provide a brief summary, key topics, and sentiment:\\n\\n\"\n",
    "#         f\"{chunk_text}\\n\\n\"\n",
    "#         \"Summary:\"\n",
    "#     )\n",
    "\n",
    "#     # Create the chat completion\n",
    "#     response = client.chat.completions.create(\n",
    "#         model='gpt-4o-mini',  # Specify the model\n",
    "#         messages=[\n",
    "#             # The conversation history, starting with the user's prompt\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#         ],\n",
    "#         max_tokens=150,        # Limit the response tokens\n",
    "#         temperature=0.25,      # Control the randomness\n",
    "#         n=1,                   # Number of responses to generate\n",
    "#         stop=None              # When to stop generating tokens\n",
    "#     )\n",
    "\n",
    "#     # Extract the assistant's reply\n",
    "#     metadata_text = response.choices[0].message.content.strip()\n",
    "#     return metadata_text\n",
    "\n",
    "# for item in tqdm(processed_letters + processed_reports):\n",
    "#     metadata = generate_metadata(item['content'])\n",
    "#     item['metadata'] = metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(chunk_text, source):\n",
    "    if source == 'letter':\n",
    "        prompt = (\n",
    "            \"You are analyzing a chunk of text from a Warren Buffett annual shareholder letter. \"\n",
    "            \"These letters often include Buffett's investment philosophies, discussions on company performance, reflections on the economy, and advice to investors. \"\n",
    "            \"Please analyze the following text and provide a very brief summary in less than 20 words and a maximum of 3-5 key topics.\"\n",
    "            \"output the results as a dictionary in plain text without any code block formatting or escape characters, the dictionary should contain the following keys: 'summary', 'key_topics'.\\n\\n\"\n",
    "\n",
    "            f\"Text:\\n{chunk_text}\\n\\n\"\n",
    "            \n",
    "            \"Output: summary: <summary>, key_topics: <key_topics>\"\n",
    "        )\n",
    "    elif source == 'report':\n",
    "        prompt = (\n",
    "            \"You are analyzing a chunk of text from a company's annual report. \"\n",
    "            \"Annual reports typically contain financial statements, management discussions, risk factors, market analysis, and future outlooks. \"\n",
    "            \"Please analyze the following text and provide a very brief summary in less than 20 words and a maximum of 3-5 key topics.\"\n",
    "            \"output the results as a dictionary in plain text without any code block formatting or escape characters, the dictionary should contain the following keys: 'summary', 'key_topics'.\\n\\n\"\n",
    "\n",
    "            f\"Text:\\n{chunk_text}\\n\\n\"\n",
    "\n",
    "            \"Output: summary: <summary>, key_topics: <key_topics>\"\n",
    "        )\n",
    "    elif source == 'paper':\n",
    "        prompt = (\n",
    "            \"You are an expert financial analyst and researcher specializing in trading strategies and behavioral finance. \"\n",
    "            \"Analyze the following excerpt from a research paper and provide a very brief summary in less than 20 words and a maximum of 3-5 key topics.\"\n",
    "            \"Output the results as a dictionary in plain text without any code block formatting or escape characters, the dictionary should contain the following keys: 'summary', 'key_topics'.\\n\\n\"\n",
    "\n",
    "            f\"Text:\\n{chunk_text}\\n\\n\"\n",
    "\n",
    "            \"Output: summary: <summary>, key_topics: <key_topics>\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"Analyze the following text and provide a brief summary, key topics.\\n\\n\"\n",
    "            f\"Text:\\n{chunk_text}\\n\\n\"\n",
    "            \"Please provide a list of Summary and Key topics as response.\"\n",
    "        )\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',  # Specify the model\n",
    "        messages=[\n",
    "            # The conversation history, starting with the user's prompt\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,        # Limit the response tokens\n",
    "        temperature=0.5,       # Control the randomness\n",
    "        n=1,                   # Number of responses to generate\n",
    "        stop=None              # When to stop generating tokens\n",
    "    )\n",
    "    metadata_text = response.choices[0].message.content.strip()\n",
    "    return metadata_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668/668 [15:03<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in tqdm(processed_papers):\n",
    "    metadata = generate_metadata(item['content'], source='paper')\n",
    "    item['metadata'] = metadata\n",
    "    \n",
    "output_path = '../data/research_papers.json'\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(processed_papers, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'paper',\n",
       " 'chunk_id': 'ssrn-4599565_chunk_0',\n",
       " 'content': 'DETECTING LEAD-LAGRELATIONSHIPS IN STOCK RETURNS AND PORTFOLIO STRATEGIES∗ Álvaro Cartea†‡Mihai Cucuringu∗§†¶Qi Jin∗‡∥ June 9, 2024 Click here for the most recent version ABSTRACT We propose a method to detect linear and nonlinear lead-lag relationships in stock returns. Our approach uses pairwise Lévy-area and cross-correlation of returns to rank the assets from leaders to followers. We use the rankings to construct a portfolio that longs or shorts the followers based on the previous returns of the leaders, and the stocks are ranked every time the portfolio is rebalanced. The portfolio also takes an offsetting position on the SPY ETF so that the initial value of the portfolio is zero. Our data spans from 1963 to 2022, and we use an average of over 500 stocks to construct portfolios for each trading day. The annualized returns of our lead-lag portfolios are over 20 %, and the returns outperform all lead-lag benchmarks in the literature. There is little overlap between the leaders and the followers we find and those that are reported in previous studies based on market capitalization, volume traded, and intra-industry relationships. Our findings support the slow information diffusion hypothesis; i.e., portfolios rebalanced once a day consistently outperform the bidiurnal, weekly, bi-weekly, tri-weekly, and monthly rebalanced portfolios. Keywords : Return prediction, Lead-lag relationships, Ranking, Lévy-area, Clustering JEL classification : G11, G12, G14, G17 ∗We thank Andrew Alden, Torben Andersen, Álvaro Arroyo, Patrick Chang, Fayçal Drissi, Anthony Ledford, Slavi Marinov, Sean Myers (discussant), Roberto Renò, and Harrison Waldon for helpful comments and feedback. We are grateful to audience at Man AHL, J.P. Morgan, GSA Capital, and Oxford Asset Management for comments. We are grateful to audience at the OMI Machine Learning and Financial Econometrics workshop and the Eastern Finance Association Annual Meeting for helpful comments. †Oxford-Man Institute of Quantitative Finance, University of Oxford ‡Mathematical Institute, University of Oxford §Department of Statistics, University of Oxford ¶The Alan Turing Institute, London, UK ∥Corresponding author; Email: qi.jin@st-annes.ox.ac.uk1 Introduction Changes in stock prices of some firms tend to follow those of other firms. This relationship between stock prices is often referred to as a lead-lag relationship. Detecting lead-lag relationships among a large set of stocks is not straightforward. The extant literature uses ad-hoc methods to select leaders and followers, and employs these two sets of stocks in investment strategies to evaluate the economic significance of the lead-lag relationship. For example, Lo and MacKinlay (1990) assume that large market capitalization stocks lead small market capitalization stocks. They build equal-weighted portfolios within each quantile of market capitalizations and use the cross-autocorrelation between the five portfolios to evaluate the trading performance of the lead-lag relationship. Empirical evidence suggests that firm size (Lo and MacKinlay (1990)), trading volume (Chordia and Swaminathan (2000)), institutional ownership (Badrinath et al. (1995)), and other firm characteristics contribute to the lead-lag identity of a stock. Empirically, however, many lead-lag relationships change over time and often cannot be explained by sorting stocks on a single firm characteristic.7This observation motivates that it is necessary to detect, instead of assume and then verify, lead-lag relationships. Our objective is to find lead-lag relationships without explicitly assuming a link between firm characteristics and lead-lag relationships; instead, we develop a data-driven method that employs stock returns to identify leaders and followers, and we show that the lead-lag relationships we find are economically significant. We achieve this in three steps. First, we design an algorithm that identifies the direction and strength of the lead-lag relationship between the returns of two stocks. Second, we propose a framework that uses state-of-the-art algorithms to rank stocks from leaders to followers based on the pairwise relationships. Third, we construct a zero-cost portfolio to assess the returns predictability of the leaders over the followers, and we measure the economic significance of the portfolio’s performance. Specifically, in the first step we design a method to score the lead-lag relationship between pairs of assets. The sign of the score indicates which of the two assets is more likely the leader, and the magnitude of the score quantifies the strength of the lead-lag relationship.',\n",
       " 'metadata': 'summary: Method detects lead-lag relationships in stock returns for superior portfolio strategies.  \\nkey_topics: Lead-lag relationships, Stock returns, Portfolio strategies, Return prediction, Economic significance'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 693/693 [13:41<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in tqdm(processed_letters):\n",
    "    metadata = generate_metadata(item['content'], source='letter')\n",
    "    item['metadata'] = metadata\n",
    "    \n",
    "output_path = '../data/buffet_letters.json'\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(processed_letters, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = processed_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'ticker', 'year', 'chunk_id', 'content'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 932/932 [03:56<00:00,  3.94it/s] \n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(processed_reports):\n",
    "\n",
    "    if 'metadata' in item.keys():\n",
    "        pass\n",
    "    else:\n",
    "        metadata = generate_metadata(item['content'], source='report')\n",
    "        item['metadata'] = metadata\n",
    "\n",
    "output_path = '../data/annual_reports.json'\n",
    "with open(output_path, 'w') as json_file:\n",
    "    json.dump(processed_reports, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in processed_letters:\n",
    "    metdt = item['metadata'].split('\\n')\n",
    "    item['summary'] = metdt[0][len(\"summary:\"):].strip()\n",
    "    item['key_topics'] = [i.strip() for i in metdt[1][len(\"key_topics:\"):].strip().split(',')]\n",
    "    del item['metadata']\n",
    "    \n",
    "for item in processed_reports:\n",
    "    \n",
    "    metdt = item['metadata'].split('\\n')\n",
    "\n",
    "    item['summary'] = metdt[0][len(\"summary:\"):].strip()\n",
    "    item['key_topics'] = [i.strip() for i in metdt[1][len(\"key_topics:\"):].strip().split(',')]\n",
    "    del item['metadata']\n",
    "\n",
    "    item['source'] = item['ticker'] + \" annual \" + item['source']\n",
    "    del item['ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Create an Elasticsearch client instance\n",
    "es = Elasticsearch(\n",
    "    [{'scheme': 'http', 'host': 'localhost', 'port': 9200}]\n",
    ")\n",
    "\n",
    "index_name = 'enhanced_stock_analyzer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_10392/3529335748.py:16: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  if not es.indices.exists(index=index_name):\n"
     ]
    }
   ],
   "source": [
    "index_mapping = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'source': {'type': 'keyword'},\n",
    "            'year': {'type': 'keyword'},\n",
    "            'chunk_id': {'type': 'keyword'},\n",
    "            'content': {'type': 'text'},\n",
    "            'summary': {'type': 'text'},\n",
    "            'key_topics': {'type': 'text'},\n",
    "            # Add more fields if needed\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=index_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_10392/2303991736.py:22: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  bulk(es, generate_actions(all_data))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1625, [])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "def generate_actions(data):\n",
    "    for item in data:\n",
    "        yield {\n",
    "            '_index': index_name,\n",
    "            '_id': item['chunk_id'],\n",
    "            '_source': {\n",
    "                'source': item['source'],\n",
    "                'year': item['year'],\n",
    "                'chunk_id': item['chunk_id'],\n",
    "                'content': item['content'],\n",
    "                'summary': item['summary'],\n",
    "                'key_topics': item['key_topics'],\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Combine all data\n",
    "all_data = processed_letters + processed_reports\n",
    "\n",
    "# Bulk index the data\n",
    "bulk(es, generate_actions(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'letter',\n",
       " 'year': '2018',\n",
       " 'chunk_id': '2018_chunk_1',\n",
       " 'content': 'Fornearly three decades, the initial paragraph featured the percentage change in Berkshire’s per-share book value. It’snow time to abandon that practice.The fact is that the annual change in Berkshire’s book value – which makes its farewell appearance on page2 – is a metric that has lost the relevance it once had. Three circumstances have made that so. First, Berkshire hasgradually morphed from a company whose assets are concentrated in marketable stocks into one whose major valueresides in operating businesses. Charlie and I expect that reshaping to continue in an irregular manner. Second, whileourequity holdingsare valued at market prices, accounting rules require our collection of operating companiesto be included in book value at an amount far below their current value, a mismark that has grown in recent years. Third, itis likely that – over time – Berkshire will be a significant repurchaser of its shares, transactions that will take place atprices above book value but below our estimate of intrinsic value. The math of such purchases is simple: Eachtransaction makes per-share intrinsic value go up, while per-share book value goes down. That combination causesthe book-value scorecard to become increasingly out of touch with economic reality.3In future tabulations of our financial results, we expect to focus on Berkshire’s market price. Markets can beextremely capricious: Just look at the 54-year history laid out on page 2. Over time, however, Berkshire’s stock pricewill provide the best measure of business performance. ************Before moving on, I want to give you some good news – reallygood news – that is not reflected in our financial statements. It concerns the management changes we made in early 2018, when Ajit Jain was put in chargeof all insurance activities and Greg Abel was given authority over all other operations. These moves were overdue.Berkshire is now far better managed than when I alone was supervising operations. Ajit and Greg have rare talents,and Berkshire blood flows through their veins.Now let’s take a look at what you own.Focus on the Forest – Forget the TreesInvestors who evaluate Berkshire sometimes obsess on the details of our many and diverse businesses – oureconomic “trees,” so to speak. Analysis of that type can be mind-numbing, given that we own a vast array ofspecimens, ranging from twigs to redwoods. A few of our trees are diseased and unlikely to be around a decade fromnow. Many others, though, are destined to grow in size and beauty.Fortunately, it’s not necessary to evaluate each tree individually to make a rough estimate of Berkshire’sintrinsic business value. That’s because our forest contains five “groves” of major importance, each of which can beappraised, with reasonable accuracy, in its entirety. Four of those groves are differentiated clusters of businesses andfinancial assets that are easy to understand. The fifth – our huge and diverse insurance operation – delivers great valueto Berkshire in a less obvious manner, one I will explain later in this letter.Before we look more closely at the first four groves, let me remind you of our prime goal in the deploymentof your capital: to buy ably-managed businesses, in whole or part, that possess favorable and durable economic characteristics. We also need to make these purchases at sensible prices.Sometimes we can buy control of companies that meet our tests. Far more often, we find the attributes weseek in publicly-traded businesses, in which we normally acquire a 5% to 10% interest. Our two-pronged approach tohuge-scale capital allocation is rare in corporate America and, at times, gives us an important advantage.In recent years, the sensible course for us to follow has been clear: Many stocks have offered far more forour money than we could obtain by purchasing businesses in their entirety. That disparity led us to buy about $43billion of marketable equities last year, while selling only $19 billion. Charlie and I believe the companies in whichwe invested offered excellent value, far exceeding that available in takeover transactions.Despite our recent additions to marketable equities, the most valuable grove in Berkshire’s forest remains themany dozens of non-insurance businesses that Berkshire controls (usually with 100% ownership and never with lessthan 80%). Those subsidiaries earned $16.8 billion last year. When we say “earned,” moreover, we are describing whatremains afterallincome taxes, interest payments, managerial compensation (whether cash or stock-based),restructuring expenses, depreciation, amortization and home-office overhead.That brand of earnings is a far cry from that frequently touted by Wall Street bankers and corporate CEOs.Too often, their presentations feature “adjusted EBITDA,” a measure that redefines “earnings” to exclude a variety ofall-too-real costs. 4For example, managements sometimes assert that their company’s stock-based compensation shouldn’t becounted as an expense. (What else could it be – a giftfrom shareholders?) And restructuring expenses? Well, maybe last year’s exact rearrangement won’t recur.',\n",
       " 'summary': \"Berkshire's focus shifts from book value to market price, emphasizing intrinsic value and effective management.\",\n",
       " 'key_topics': ['investment philosophy',\n",
       "  'management changes',\n",
       "  'market price vs. book value',\n",
       "  'business performance',\n",
       "  'capital allocation']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_letters[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'AMARAJABAT annual report',\n",
       " 'year': '2022',\n",
       " 'chunk_id': 'AMARAJABAT_2022_chunk_10',\n",
       " 'content': 'Some of them were: l Connecting conveyors from formation to finishing sections to eliminate movement in the process (ABD Unit 1) l COS unloading automation by ROBOl Launched a Real-time battery traceability system (ABD-I) l Introduced a Paperless system for elimination of Log sheets (ABD-II) l Invested in Battery Readings automation in Formation section (MVRLA) l Commissioned an auto finishing line to improve productivity & reduce fatigue (LVRLA) l Started an auto scheduling system from barcode (ABD-II) l Initiated Multi-layer formation (SBD-1) Improving Productivity The Company carried on with its dedicated and focused energy conservation efforts through upgrading of process technology, effective production scheduling and various energy-saving initiatives including installation of energy efficient equipment. Few initiatives are: l Implemented superior energy- saving practices in all equipment and processes, including plant lightingl Installed BLDC fans for AHUs l Installed Auto descaling system for condensers of chillers l Improved the power factor at SDB level l Replaced old conventional chargers with improved IGBT chargers l Reduced the demand for Compressed air l Effective utilisation of reduced unit rates during off-peak hours by thermal storagel Scheduled non-continuous operating machines during off- peak hours and normal hours and avoided on-peak hour operations l Reduced skin temperature of lead melting pots l Optimised process parameters and cycle times l Conducted awareness sessions on energy conservation to suppliers l Trained employees on ISO 50001:2018 and energy savings techniques Renewable energy initiatives l Rooftop solar installations in manufacturing plants at Tirupati l Rooftop solar installations for the parking area in Tirupati Reward & recognition l Received the Excellent Energy Efficient Unit award from CII, during the 22nd National Awards for Excellence in Energy management 2021 l Bagged the Innovative Project Award from CII during 22nd National awards for Excellence in Energy management for one of the energy- saving projects on “Pneumatic cylinder size optimisation” 32 TRANSFORMING PERFORMING&Quality Management The quality ethos at Amara Raja is embodied in its articulated philosophy - Gotta Be A Better Way. Like an inspirational mantra, this propels the entire team to push the efficiency bar higher. This culture practiced for years has now integrated with the organisational DNA, which inspires the Company to strive to seek improved methods of performing the tasks. Amara Raja believes in compliance, excellence & sustainability. The endeavor to build world class products enables the Company to build on quality management systems.Securing certifications of ISO 9001 and IATF 16949 for all the facilities and improving the maturity level of Quality Management System (QMS) to serve the customer better improved business performance. The imbibed culture drives every team member to transform every transaction with Amara Raja into a heart-warming experience. Learning is a journey towards excellence and continues to effectively enhance the competency of people by conducting need-based trainings to do the job better. To embed the passion for quality across the organisation, the Company has institutionalised globally accepted operational tools and techniques namely Continuous Improvement (CI), Lean Implementation programs - TPM, Poka Yoke, Single Minute Exchange of Dies(SMED), Visual Management, 5S, work place ergonomics, Industrial Engineering (IE) studies & Lean Six Sigma and Quality Circle concepts. Encouraging employees to undertake trainings and certification courses in the quality management tools and engaging consultants to inculcate next and best practices across the business are making a visible difference in upping the quality commitment. 33 AMARA RAJA BATTERIES LIMITED ANNUAL REPORT 2021-22CORPORATE OVERVIEW02STATUTORY REPORTS14FINANCIAL STATEMENTS150MANAGEMENT DISCUSSION & ANALYSIS Key initiatives in 2021-22 The Company continued with its principles of upping its quality commitment by implementing several initiatives. They are: 1. Emphasising on quality culture through World Quality Month celebrations across all manufacturing sites & remote locations. 2. Developed E-learning modules for 5S (work place Management) and Lean awareness for improving the learnability. 5900 employees have completed the course through Alt- learning portal. 3. Standardised and sustained the usage of E modules for QCC, Suggestion Scheme and Six Sigma programmes. 4. Enhanced the lean momentum by participating in national level competitions. 5. Programmes conducted to institutionalise the focus on Ergonomics by conducting assessments to enhance the process. 5S, Quality Circles & Lean Six Sigma In 2021-2022, 601 quality circles completed 1100 QC projects. To embed the Six Sigma approach throughout the organisation, the Company trained 306 Black Belts and 1023 Green Belts, and successfully completed 3412 projects as of March 31, 2022. This task force has provided valuable insight into simplifying operations and increasing product uniformity.',\n",
       " 'summary': 'Company focuses on productivity, energy conservation, and quality management improvements.',\n",
       " 'key_topics': ['Productivity initiatives',\n",
       "  'Energy conservation',\n",
       "  'Quality management systems']}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_reports[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whysocurious/.local/share/virtualenvs/buffett-wisdom-rag-yofaZaKX/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0443a1d9be94dd9a218ce37e80bd701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03866ec6dfb84e8e967e06a564c55cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59b46cf36a14088b41efa54d801dbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b2c557cb614549902cc0d99d4080bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27049b1266564f3598b271ea16f16e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633051a0857f4b28aaae39f522610b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157ab0c040804c669e48f9859ff0878d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0af5421804640885a9a505ba38e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bacfb1e6f3f4174b0eb094883936020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91751146a1247619b1a0d52e8831aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whysocurious/.local/share/virtualenvs/buffett-wisdom-rag-yofaZaKX/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be292b97accb48739d78ac0d7bcbccfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for item in all_data:\n",
    "    embedding = model.encode(item['content'])\n",
    "    item['embedding'] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname='buffet_wisdom_v1',\n",
    "    user='whysocurious',\n",
    "    password='buffet_v1',\n",
    "    host='localhost',\n",
    "    port='5432'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "cursor.execute(\"DROP TABLE IF EXISTS embeddings\")\n",
    "# Create table\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "        chunk_id TEXT PRIMARY KEY,\n",
    "        source TEXT,\n",
    "        year TEXT,\n",
    "        summary TEXT,\n",
    "        keytopics TEXT[],\n",
    "        embedding FLOAT[]\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Insert embeddings\n",
    "for item in all_data:\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO embeddings (chunk_id, source, year, summary, keytopics, embedding)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (chunk_id) DO NOTHING;\n",
    "    \"\"\", (\n",
    "        item['chunk_id'],\n",
    "        item['source'],\n",
    "        item['year'],\n",
    "        item['summary'],\n",
    "        item['key_topics'],\n",
    "        item['embedding'].tolist(),  # Convert numpy array to list\n",
    "    ))\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buffett-wisdom-rag-yofaZaKX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
